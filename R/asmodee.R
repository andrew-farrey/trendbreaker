#' Automatic Selection of Models Outlier DEtection for Epidemics (ASMODEE)
#'
#' This function implements an algorithm for epidemic time series analysis in
#' aim to detect recent deviation from the trend followed by the data. Data is
#' first partitioned into 'recent' data, using the last `k` observations as
#' supplementary individuals, and older data used to fit the
#' trend. Trend-fitting is done by fitting a series of user-specified models for
#' the time series, with different methods for selecting best fit (see details,
#' and the argument `method`). The prediction interval is then calculated for
#' the best model, and every data point (including the training set and
#' supplementary individuals) falling outside are classified as 'outliers'.
#'
#' @details Automatic model selection is used to determine the model best
#'   fitting the training data from a list of user-provided models. First, all
#'   models are fitted to the data. Second, models are selected using the
#'   approach specified by the `method` argument. The default is
#'   [`evaluate_aic()`] which uses Akaike's Information Criteria to assess model
#'   fit penalised by model complexity. This approach is fast, but only measures
#'   model fit rather than predictive ability. The alternative is using
#'   [`evaluate_resampling()`], uses cross-validation (10-fold by default) and
#'   root mean squared error (RMSE) to assess model fit. This approach is likely
#'   to select models with good predictive abilities, but is computationally
#'   intensive.
#'
#' @author Thibaut Jombart, Dirk Schumacher and Tim Taylor, with inputs from
#'   Michael Höhle, Mark Jit, John Edmunds, Andre Charlett, Stéphane Ghozzi
#'
#' @param data A `data.frame` or a `tibble` containing the response and
#'   explanatory variables used in the `models`.
#' @param models A list of [`trending_model()`] objects,
#'   generated by `lm_model`, `glm_model`, `glm_nb_model`, `brms_model` and
#'   similar functions (see `?trending::trending_model()`) for details.
#' @param ... Further arguments passed to `method`.
#'
#' @return An `trendbreaker` object (S3 class inheriting `list`), containing items
#'   which can be accessed by various accessors - see `?trendbreaker-accessors`
#'
#' @examples
#' if (require(cowplot) && require(tidyverse) && require(trending)) {
#'   # load data
#'   data(nhs_pathways_covid19)
#'
#'   # select last 28 days
#'   first_date <- max(nhs_pathways_covid19$date, na.rm = TRUE) - 28
#'   pathways_recent <- nhs_pathways_covid19 %>%
#'     filter(date >= first_date)
#'
#'   # define candidate models
#'   models <- list(
#'     regression = lm_model(count ~ day),
#'     poisson_constant = glm_model(count ~ 1, family = "poisson"),
#'     negbin_time = glm_nb_model(count ~ day),
#'     negbin_time_weekday = glm_nb_model(count ~ day + weekday)
#'   )
#'
#'   # analyses on all data
#'   counts_overall <- pathways_recent %>%
#'     group_by(date, day, weekday) %>%
#'     summarise(count = sum(count))
#'
#'   # results with fixed value of 'k' (7 days)
#'   res_overall_k7 <- asmodee(counts_overall, models, date, k = 7)
#'   plot(res_overall_k7, "date")
#' }
#'
#' @rdname asmodee
#' @export
#'
asmodee <- function(data, models, ...) {
  UseMethod("asmodee", data)
}


#' @param date_index The name of a variable corresponding to time, quoted or
#'   not.
#' @param alpha The alpha threshold to be used for the prediction interval
#'   calculation; defaults to 0.05, i.e. 95% prediction intervals are
#'   calculated.
#' @param k An `integer` indicating the number of recent data  points to be
#'   excluded from the trend fitting procedure. Defaults to 1.
#' @param method A function used to evaluate model fit. Current choices are
#'   `evaluate_aic` (default) and `evaluate_resampling`. `evaluate_aic` uses
#'   Akaike's Information Criterion instead, which is faster but possibly less
#'   good a selecting models with the best predictive power.
#'   `evaluate_resampling` uses cross-validation and, by default, RMSE to assess
#'   model fit.
#' @param simulate_pi Should the ciTools package be used to simulate prediction
#'   intervals for glm models. Defaults to TRUE.
#' @param uncertain Only used for glm models and if simulate_pi = FALSE. If
#'   FALSE uncertainty in the fitted parameters is ignored when generating the
#'   prediction intervals. Defaults to FALSE.
#' @param include_fitting_warnings Should results include models that triggered
#'   warnings, but not errors, during the fitting procedure. Defaults to
#'   `FALSE`.
#' @param include_prediction_warnings Should results include models that
#'   triggered warnings, but not errors, during the prediciton stage. Defaults
#'   to `FALSE`.
#' @param force_positive A `logical` indicating if prediction should be forced
#'   to be positive (or zero); can be useful when using Gaussian models for
#'   count data, to censor confidence or prediction intervals and avoid
#'   negative predictions. Defaults to `FALSE` for general `data.frame` inputs,
#'   and to `TRUE` for `incidence2` objects.
#' @param quiet A `logical` indicating if warnings and messages should be
#'   suppressed (TRUE) or use (FALSE, default).
#' @param keep_intermediate A `logical` indicating if all output from the
#'   fitting and prediction stages should be kept. If `TRUE` then a tibble will
#'   be returned in the .fitted_results position of the resulting list output.
#'   If `FALSE` (default) .fitted_results will be `NULL`.
#'
#' @rdname asmodee
#' @export
asmodee.data.frame <- function(data, models, date_index, alpha = 0.05, k = 1,
                               method = evaluate_aic,
                               simulate_pi = TRUE, uncertain = FALSE,
                               include_fitting_warnings = FALSE,
                               include_prediction_warnings = TRUE,
                               force_positive = FALSE,
                               quiet = FALSE,
                               keep_intermediate = FALSE, ...) {

  # sanity checks
  # TODO - may want to add more
  if (!length(models)) stop("models has a length of zero")
  if (!is.numeric(k) || !is.finite(k)) {
    stop("`k` must be a finite number")
  }

  # As the method relies on a 'time' variable for defining training/testing
  # sets, we first need to retrieve this information from the 'time_index'
  # argument. We borrow the same strategy as the one used in the *incidence2*
  # package.
  date_index <- rlang::enquo(date_index)
  idx <- tidyselect::eval_select(date_index, data)
  date_index <- names(data)[idx]
  dates <- data[[date_index]]

  # Ensure k is of "reasonable" size
  n_dates <- length(unique(dates))
  if (k > (n_dates - 4)) {
    msg <- sprintf("`k` (%d) is too high for the dataset size (%d)", k, n_dates)
    stop(msg)
  }

  # The rest of the algorithm will basically rely on:
  #  1. defining the training set by removing data of the most recent 'k' time
  #     units in date_index; this is externalised in get_training_data()
  #  2. fitting the model
  #  3. removing models that error (or optionally give warnings)
  #  4. evaluate using the training set (potential duplication here)
  #  4. deriving prediction intervals
  #  6. and classifying outliers

  # Define the training set and boundary dates
  # TODO - this can be streamlined quite a bit as duplicating code
  selected_k <- as.integer(max(k, 0L))
  data <- set_training_data(data, date_index, selected_k)
  training <- data$training
  last_training_date <- max(dates[training], na.rm = TRUE)
  first_testing_date <- NULL
  if (selected_k > 0) {
    first_testing_date <- min(dates[!training], na.rm = TRUE)
  }
  data_train <- get_training_data(data, date_index, selected_k)

  # Here we eliminate models which would error when using predict on
  # the testing set due to either new levels in the prediction set for
  # categorical predictors (factors), or the presence of NAs in the predictors.
  models <- retain_sanitized_models(
    models,
    x_training = data_train,
    x_testing = data,
    warn = !quiet,
    error_if_void = TRUE
  )

  # fit all of the models and capture the warnings and errors
  fitted_results <- lapply(models, function(x) safely(trending::fit)(x, data_train))
  fitted_results <- base_transpose(fitted_results)
  names(fitted_results) <- c("trending_model_fit", "fitting_warnings", "fitting_errors")
  fitted_results <- tibble::as_tibble(fitted_results)

  # keep fitting_results (optional). useful for debugging
  .fitted_results <- NULL
  if (keep_intermediate) .fitted_results <- fitted_results

  # remove fitting errors
  # TODO - make function for the keeping
  keep <- vapply(fitted_results$fitting_errors, is.null, logical(1))
  fitted_results <- fitted_results[keep,]
  models <- models[keep]

  # remove fitting warnings (optionally)
  if (!include_fitting_warnings) {
    keep <- vapply(fitted_results$fitting_warnings, is.null, logical(1))
    fitted_results <- fitted_results[keep,]
    models <- models[keep]
  }

  # evaluate the models
  # TODO - some duplication here
  model_results <- trendeval::evaluate_models(
    data = data_train,
    models = models,
    method = method,
    ...
  )
  model_results$model <- NULL   # this is cleaning up from trendeval output
  model_results$data <- NULL    # this is cleaning up from trendeval output
  model_results <- dplyr::rename(
    model_results,
    evaluation_warnings = warning,
    evaluation_errors = error
  )

  # combine fitting, evaluation data
  out <- dplyr::bind_cols(fitted_results, model_results)

  # bring model_name to front (here we allow for potential of unnamed models)
  if (!"model_name" %in% names(out)) {
    out$model_name <- NA_character_
  }
  out <- dplyr::relocate(out, model_name)

  # In case there are any errors with evaluation we remove these here also
  # This whould not occur with `evaluate_aic` as models causing problems
  # would already have been removed. Mainly for evaluating methods that rely
  # on sampling
  keep <- vapply(out$evaluation_errors, is.null, logical(1))
  out <- out[keep,]
  models <- models[keep]

  # error if no possible models
  if (nrow(out) == 0) {
    if (include_fitting_warning) {
      msg <- "Unable to fit a model to the data without warnings or errors."
    } else {
      msg <- paste(
        "Unable to fit a model to the data without warnings or errors.",
        "Consider using `include_fitting_warnings = TRUE`",
        "to include more models which issued warnings.",
        sep = "\n"
      )
    }
    stop(msg)
  }

  # order results and keep the top one
  # TODO - this assumes only last column is of interest. Ok for aic but would
  #        be an issue if people passed multiple metrics through to a different
  #        method so we should warn in this scenario
  models <- models[order(out[, ncol(out), drop = TRUE])]
  out <- out[order(out[, ncol(out), drop = TRUE]), ]

  # loop through possible models until we can succesfully make prediction
  i <- 0
  success <- FALSE
  tmp <- out$trending_model_fit
  while(i < length(tmp) && !success) {
    i <- i + 1
    pred_result <- safely(predict)(
      tmp[[i]],
      new_data = data,
      alpha = alpha,
      simulate_pi = simulate_pi,
      uncertain = uncertain
    )
    names(pred_result) <- c("result", "prediction_warnings", "prediction_errors")
    pred_result <- lapply(pred_result, list)
    pred_result <- tibble::as_tibble(pred_result)

    # remove prediction errors
    keep <- vapply(pred_result$prediction_errors, is.null, logical(1))
    pred_result <- pred_result[keep,]
    if (nrow(pred_result) == 0) next

    # remove prediction warnings (optionally)
    if (!include_prediction_warnings) {
      keep <- vapply(pred_result$prediction_warnings, is.null, logical(1))
      pred_result <- pred_result[keep,]
    }

    if (nrow(pred_result) == 0) next
    success <- TRUE
  }

  # error if no possible models
  if (!success) {
    if (include_prediction_warning) {
      msg <- "Unable to make prediction to the data without warnings or errors."
    } else {
      msg <- paste(
        "Unable to make a prediction to the data without warnings or errors.",
        "Consider using `include_prediction_warnings = TRUE`",
        "to include more models which issued warnings.",
        sep = "\n"
      )
    }
    stop(msg)
  }

  # subset to the best model and then combine with prediction results
  out <- out[i, ]
  models <- models[i]

  # combine fitting, evaluation data
  out <- dplyr::bind_cols(out, pred_result)

  # mark up outliers
  preds <- out$result[[1]]
  model <- out$trending_model_fit[[1]]$fitted_model
  observed <- all.vars(formula(model))[1]
  outliers <- dplyr::mutate(
    preds,
    outlier = .data[[observed]] < .data$lower_pi | .data[[observed]] > .data$upper_pi,
    classification = dplyr::case_when(
      .data[[observed]] < .data$lower_pi ~ "decrease",
      .data[[observed]] > .data$upper_pi ~ "increase",
      TRUE ~ "normal"
    ),
    classification = factor(.data$classification,
                            levels = c("increase", "normal", "decrease")
    )
  )

  # enforce positive predictions if required
  if (force_positive) {
    nms <- c("estimate", "lower_ci", "upper_ci", "lower_pi", "upper_pi")
    outliers[nms] <- lapply(outliers[nms], neg_to_zero)
  }

  # final output
  out <- list(
    k = selected_k,
    model_name = out$model_name[[1]],
    trending_model_fit = out$trending_model_fit[[1]],
    alpha = alpha,
    results = outliers,
    date_index = date_index,
    last_training_date = last_training_date,
    first_testing_date = first_testing_date,
    .fitted_results = .fitted_results
  )
  class(out) <- c("trendbreaker", class(out))
  out
}


#' @rdname asmodee
#' @param include_group_warnings Should results include models that triggered
#'   warnings, but not errors, during the prediciton stage. Defaults to `FALSE`.
#'
#' @export
asmodee.incidence2 <- function(data, models, alpha = 0.05, k = 1,
                               method = evaluate_aic, simulate_pi = TRUE,
                               uncertain = FALSE,
                               include_fitting_warnings = FALSE,
                               include_prediction_warnings = TRUE,
                               include_group_warnings = FALSE,
                               force_positive = TRUE, quiet = FALSE,
                               keep_intermediate = FALSE, ...) {

  # check incidence2 package is present
  check_suggests("incidence2")

  groups <- incidence2::get_group_names(data)
  if (!is.null(groups)) {
    f_groups <- lapply(suppressMessages(data[groups]), factor, exclude = NULL)
    split_dat <- split(data, f_groups, sep = "-")
  } else {
    split_dat <- list(data)
  }
  date_index <- incidence2::get_dates_name(data)

  out <- lapply(
    split_dat,
    safely(asmodee.data.frame),
    models = models,
    date_index = date_index,
    alpha = alpha,
    k = k,
    method = method,
    simulate_pi = simulate_pi,
    uncertain = uncertain,
    include_fitting_warnings = include_fitting_warnings,
    include_prediction_warnings = include_prediction_warnings,
    force_positive = force_positive,
    quiet = quiet,
    keep_intermediate = keep_intermediate
  )

  out <- base_transpose(out)
  nms <- names(split_dat)
  if (is.null(nms)) nms <- NA
  out <- tibble::tibble(
    group = nms,
    output = out[[1]],
    warnings = out[[2]],
    errors = out[[3]]
  )

  # remove errors
  keep <- vapply(out$errors, is.null, logical(1))
  out <- out[keep,]

  # optionally remove warnings
  if (!include_group_warnings) {
    keep <- vapply(out$warnings, is.null, logical(1))
    out <- out[keep,]
  }

  class(out) <- c("trendbreaker_incidence2", class(out))
  out
}


neg_to_zero <- function(x) {
  x[x < 0] <- 0
  x
}
